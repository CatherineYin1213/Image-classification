{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31d18200",
   "metadata": {},
   "source": [
    "# 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f85b2b43",
   "metadata": {},
   "source": [
    "#导入相关库\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "from imageio import imread"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df1b0f66",
   "metadata": {},
   "source": [
    "#设置整体的图片显示\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12.0, 10.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12f37cbb",
   "metadata": {},
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0d51641",
   "metadata": {},
   "source": [
    "#定义加载CIFAR数据集的函数\n",
    "#加载单个batch\n",
    "def load_CIFAR_batch(filename):\n",
    "    with open(filename,'rb') as f:\n",
    "        datadict = pickle.load(f,encoding='latin1')\n",
    "        X = datadict['data']\n",
    "        Y = datadict['labels']\n",
    "        X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
    "        Y = np.array(Y)\n",
    "        return X, Y\n",
    "def load_CIFAR10(ROOT):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for b in range(1,6):\n",
    "        f = os.path.join(ROOT,'data_batch_%d'%(b, ))\n",
    "        X,Y = load_CIFAR_batch(f)\n",
    "        xs.append(X)\n",
    "        ys.append(Y)\n",
    "    Xtr = np.concatenate(xs)\n",
    "    Ytr = np.concatenate(ys)\n",
    "    del X,Y\n",
    "    Xte, Yte = load_CIFAR_batch(os.path.join(ROOT,'test_batch'))\n",
    "    return Xtr,Ytr,Xte,Yte"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a073394e",
   "metadata": {},
   "source": [
    "#将CIFAR-10的数据导入\n",
    "cifar10_dir = 'cifar-10-batches-py'\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "#打印CIFAR-10的训练集和测试集\n",
    "print('训练集数据大小：', X_train.shape)\n",
    "print('训练集标签大小：',y_train.shape)\n",
    "print('测试集数据大小：',X_test.shape)\n",
    "print('测试集标签大小：',y_test.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d23d8108",
   "metadata": {},
   "source": [
    "classes = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "num_class = len(classes)\n",
    "samples_per_class = 10\n",
    "for y,cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(y_train == y)\n",
    "    idxs = np.random.choice(idxs,samples_per_class,replace = False)\n",
    "    for i,idx in enumerate(idxs):\n",
    "        plt_idx = i*num_class+y+1\n",
    "        plt.subplot(samples_per_class,num_class,plt_idx)\n",
    "        plt.imshow(X_train[idx].astype('uint8'))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8414ad08",
   "metadata": {},
   "source": [
    "#将CIFAR-10的训练集划分为训练集、验证集、开发集，测试集仍全部保留为测试集\n",
    "num_training = 46000\n",
    "num_validation = 2000\n",
    "num_test = 10000\n",
    "num_dev = 10000\n",
    "\n",
    "#划分验证集\n",
    "X_val = X_train[num_training:num_training+num_validation]\n",
    "y_val = y_train[num_training:num_training+num_validation]\n",
    "\n",
    "#划分训练集\n",
    "X_train = X_train[:num_training]\n",
    "y_train = y_train[:num_training]\n",
    "\n",
    "#划分开发集\n",
    "indices = np.random.choice(num_training,num_dev,replace = False)\n",
    "X_dev = X_train[indices]\n",
    "y_dev = y_train[indices]\n",
    "\n",
    "#测试集保持不变\n",
    "X_test = X_test[:num_test]\n",
    "y_test = y_test[:num_test]\n",
    "\n",
    "# 输出各数据集的形状\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Development data shape: ', X_dev.shape)\n",
    "print('Development labels shape: ', y_dev.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a7b08d2",
   "metadata": {},
   "source": [
    "#数据预处理\n",
    "X_train = np.reshape(X_train,(X_train.shape[0],-1))\n",
    "X_val = np.reshape(X_val,(X_val.shape[0],-1))\n",
    "X_test = np.reshape(X_test,(X_test.shape[0],-1))\n",
    "X_dev = np.reshape(X_dev,(X_dev.shape[0],-1))\n",
    "\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2c8913a",
   "metadata": {},
   "source": [
    "#减去均值图像\n",
    "#首先计算训练集的均值图像（防止数据泄露+模拟实际应用场景，保持验证集和测试集的独立性+独立同分布假设）\n",
    "mean_image = np.mean(X_train,axis = 0)\n",
    "print(mean_image[:10])\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(mean_image.reshape(32,32,3).astype('uint8'))\n",
    "plt.show()\n",
    "\n",
    "#减去均值图像\n",
    "X_train -= mean_image\n",
    "X_val -= mean_image\n",
    "X_test -= mean_image\n",
    "X_dev -= mean_image\n",
    "\n",
    "#添加偏置项\n",
    "X_train = np.hstack([X_train,np.ones((X_train.shape[0],1))])\n",
    "X_val = np.hstack([X_val,np.ones((X_val.shape[0],1))])\n",
    "X_test = np.hstack([X_test,np.ones((X_test.shape[0],1))])\n",
    "X_dev = np.hstack([X_dev,np.ones((X_dev.shape[0],1))])\n",
    "\n",
    "print(X_train.shape,X_val.shape,X_test.shape,X_dev.shape)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f6a2b0de",
   "metadata": {},
   "source": [
    "# 线性SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e349371",
   "metadata": {},
   "source": [
    "class LinearSVM:\n",
    "    def __init__(self):\n",
    "        self.W = None  # 权重矩阵\n",
    "\n",
    "    def loss(self, X, y, reg):\n",
    "        num_train = X.shape[0]\n",
    "        scores = X.dot(self.W)\n",
    "        correct_class_scores = scores[np.arange(num_train), y].reshape(-1, 1)\n",
    "        margins = np.maximum(0, scores - correct_class_scores + 1)\n",
    "        margins[np.arange(num_train), y] = 0\n",
    "        loss = np.sum(margins) / num_train + 0.5 * reg * np.sum(self.W**2)\n",
    "\n",
    "        inter_mat = np.zeros_like(scores)\n",
    "        inter_mat[margins > 0] = 1\n",
    "        inter_mat[np.arange(num_train), y] = -np.sum(inter_mat, axis=1)\n",
    "        dW = X.T.dot(inter_mat) / num_train + reg * self.W\n",
    "\n",
    "        return loss, dW\n",
    "\n",
    "    def train(self, X, y, learning_rate, reg, num_iters, batch_size, verbose=False):\n",
    "        num_train, dim = X.shape\n",
    "        num_classes = np.max(y) + 1\n",
    "        if self.W is None:\n",
    "            self.W = 0.001 * np.random.randn(dim, num_classes)\n",
    "        \n",
    "        loss_history = []\n",
    "        for it in range(num_iters):\n",
    "            idx_batch = np.random.choice(num_train, batch_size, replace=True)\n",
    "            X_batch = X[idx_batch]\n",
    "            y_batch = y[idx_batch]\n",
    "            \n",
    "            loss, grad = self.loss(X_batch, y_batch, reg)\n",
    "            loss_history.append(loss)\n",
    "            self.W -= learning_rate * grad\n",
    "            \n",
    "            if verbose and it % 100 == 0:\n",
    "                print(f'Iteration {it} / {num_iters}: Loss {loss}')\n",
    "        return loss_history\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.argmax(X.dot(self.W), axis=1)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "887e26f2",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5, 1e-6, 1e-7]\n",
    "regularizations = [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4]\n",
    "num_iters = [500, 1000, 1500, 2000]\n",
    "batch_size = 200\n",
    "\n",
    "results = {}\n",
    "total_configurations = len(learning_rates) * len(regularizations) * len(num_iters)\n",
    "progress_bar = tqdm(total=total_configurations, desc=\"Training progress\")\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for reg in regularizations:\n",
    "        svm = LinearSVM()\n",
    "        loss_history = []\n",
    "        train_accuracies = []\n",
    "        val_accuracies = []\n",
    "        for iters in num_iters:\n",
    "            loss_hist = svm.train(X_train, y_train, learning_rate=lr, reg=reg, num_iters=iters, batch_size=batch_size, verbose=False)\n",
    "            y_train_pred = svm.predict(X_train)\n",
    "            y_val_pred = svm.predict(X_val)\n",
    "            train_accuracy = np.mean(y_train_pred == y_train)\n",
    "            val_accuracy = np.mean(y_val_pred == y_val)\n",
    "            loss_history.append(loss_hist[-1])\n",
    "            train_accuracies.append(train_accuracy)\n",
    "            val_accuracies.append(val_accuracy)\n",
    "            progress_bar.update(1)\n",
    "        results[(lr, reg)] = (num_iters, loss_history, train_accuracies, val_accuracies)\n",
    "\n",
    "progress_bar.close()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14c09718",
   "metadata": {},
   "source": [
    "# 创建三组子图，每组对应一个指标：损失、训练准确率、验证准确率\n",
    "fig, axes = plt.subplots(3, len(learning_rates), figsize=(10 * len(learning_rates), 10 * 3))  # 使每个子图都是方形\n",
    "\n",
    "for idx_lr, lr in enumerate(learning_rates):\n",
    "    for idx_metric, metric in enumerate(['Loss', 'Train Acc', 'Val Acc']):\n",
    "        ax = axes[idx_metric, idx_lr]\n",
    "        for reg in regularizations:\n",
    "            iters, loss_history, train_accuracies, val_accuracies = results[(lr, reg)]\n",
    "            if metric == 'Loss':\n",
    "                values = loss_history\n",
    "                marker = 'o'\n",
    "                linestyle = '-'\n",
    "            elif metric == 'Train Acc':\n",
    "                values = train_accuracies\n",
    "                marker = '^'\n",
    "                linestyle = '--'\n",
    "            else:\n",
    "                values = val_accuracies\n",
    "                marker = 's'\n",
    "                linestyle = '-.'\n",
    "\n",
    "            label = f'Reg={reg}'\n",
    "            ax.plot(iters, values, marker=marker, linestyle=linestyle, label=label)\n",
    "        \n",
    "        ax.set_title(f'{metric} for LR={lr}')\n",
    "        ax.set_xlabel('Number of Iterations')\n",
    "        ax.set_ylabel(metric)\n",
    "\n",
    "        # 将图例放在图的外部右侧\n",
    "        ax.legend(loc='upper left', bbox_to_anchor=(1.05, 1), title='Regularization')\n",
    "\n",
    "# 调整布局以容纳图例\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(right=0.85)  # 留出更多空间在右侧给图例\n",
    "plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a1ebf27",
   "metadata": {},
   "source": [
    "# 初始化最佳性能指标和最佳模型变量\n",
    "best_val_accuracy = 0\n",
    "best_svm_model = None\n",
    "best_lr = None\n",
    "best_reg = None\n",
    "best_iters = None\n",
    "\n",
    "# 遍历所有结果，寻找具有最高验证集精度的模型\n",
    "for (lr, reg), (iters, loss_history, train_accuracies, val_accuracies) in results.items():\n",
    "    max_idx = np.argmax(val_accuracies)  # 找到最高验证精度的索引\n",
    "    if val_accuracies[max_idx] > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracies[max_idx]\n",
    "        best_lr = lr\n",
    "        best_reg = reg\n",
    "        best_iters = iters[max_idx]  # 选择导致最高验证精度的迭代次数\n",
    "        \n",
    "        # 重新训练模型以获取最佳模型\n",
    "        best_svm_model = LinearSVM()\n",
    "        best_svm_model.train(X_train, y_train, learning_rate=lr, reg=reg, num_iters=best_iters, batch_size=batch_size, verbose=False)\n",
    "\n",
    "print(f\"Best model found with LR={best_lr}, Reg={best_reg}, Iters={best_iters}, Validation Accuracy={best_val_accuracy}\")\n",
    "\n",
    "# 现在可以在测试集上评估这个最佳模型\n",
    "y_test_pred = best_svm_model.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('Linear SVM on raw pixels final test set accuracy: %f' % test_accuracy)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a02950fd",
   "metadata": {},
   "source": [
    "# SVM+HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f42f5afc",
   "metadata": {},
   "source": [
    "#SVM+HOG\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from math import sqrt, atan2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def getHOGfeat(image, stride=8, orientations=8, pixels_per_cell=(8, 8), cells_per_block=(2, 2)):\n",
    "    # 初始化参数\n",
    "    cx, cy = pixels_per_cell\n",
    "    bx, by = cells_per_block\n",
    "    sx, sy = image.shape\n",
    "    gx = np.zeros(image.shape, dtype=np.float32)\n",
    "    gy = np.zeros(image.shape, dtype=np.float32)\n",
    "    eps = 1e-5\n",
    "\n",
    "    # 计算梯度 gx 和 gy\n",
    "    gx[:, 1:-1] = image[:, 2:] - image[:, :-2]\n",
    "    gy[1:-1, :] = image[2:, :] - image[:-2, :]\n",
    "    magnitude = np.sqrt(gx**2 + gy**2)\n",
    "    orientation = np.rad2deg(np.arctan2(gy, gx + eps)) % 360\n",
    "\n",
    "    # 初始化方向直方图\n",
    "    orientation_histogram = np.zeros((int(sx / cx), int(sy / cy), orientations))\n",
    "    for i in range(orientations):\n",
    "        # 处理每个方向\n",
    "        temp_orientation = np.where((orientation >= (i * 360 / orientations)) & \n",
    "                                    (orientation < ((i + 1) * 360 / orientations)), \n",
    "                                    magnitude, 0)\n",
    "        for r in range(int(sx / cx)):\n",
    "            for c in range(int(sy / cy)):\n",
    "                orientation_histogram[r, c, i] = temp_orientation[r*cx:(r+1)*cx, c*cy:(c+1)*cy].sum()\n",
    "\n",
    "    # 归一化特征块\n",
    "    n_cellsx, n_cellsy = int(sx / cx), int(sy / cy)\n",
    "    n_blocksx, n_blocksy = (n_cellsx - bx + 1), (n_cellsy - by + 1)\n",
    "    normalised_blocks = np.zeros((n_blocksy, n_blocksx, by * bx * orientations))\n",
    "    for x in range(n_blocksx):\n",
    "        for y in range(n_blocksy):\n",
    "            block = orientation_histogram[y:y + by, x:x + bx, :].flatten()\n",
    "            normalised_blocks[y, x, :] = block / np.sqrt(np.sum(block**2) + eps)\n",
    "\n",
    "    return normalised_blocks.ravel()\n",
    "\n",
    "def extract_hog_features(images):\n",
    "    hog_features = []\n",
    "    for image in tqdm(images, desc=\"Extracting HOG features\"):\n",
    "        if len(image) == 3073:  # Assuming the last element is extra and needs to be removed\n",
    "            image = image[:-1]  # Remove the last element\n",
    "        image = image.reshape(32, 32, 3)  # Reshape flat array into 32x32x3 RGB image\n",
    "        if image.dtype != np.uint8:\n",
    "            image = image.astype(np.uint8)  # Ensure the image type is uint8\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)  # Convert to grayscale\n",
    "        image = cv2.resize(image, (64, 64))  # Resize image to 64x64 for HOG\n",
    "        hog_feature = getHOGfeat(image)\n",
    "        hog_features.append(hog_feature)\n",
    "    return np.array(hog_features)\n",
    "\n",
    "# 提取HOG特征\n",
    "X_train_hog = extract_hog_features(X_train)\n",
    "X_val_hog = extract_hog_features(X_val)\n",
    "X_test_hog = extract_hog_features(X_test)\n",
    "\n",
    "# 合并原始像素特征和HOG特征\n",
    "X_train_combined = np.hstack([X_train_hog, X_train])\n",
    "X_val_combined = np.hstack([X_val_hog, X_val])\n",
    "X_test_combined = np.hstack([X_test_hog, X_test])\n",
    "\n",
    "# 特征标准化\n",
    "scaler = StandardScaler()\n",
    "X_train_combined = scaler.fit_transform(X_train_combined)\n",
    "X_val_combined = scaler.transform(X_val_combined)\n",
    "X_test_combined = scaler.transform(X_test_combined)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef577f32",
   "metadata": {},
   "source": [
    "import joblib\n",
    "joblib.dump(scaler, 'scaler.pkl')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9de904c1",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# 初始化模型\n",
    "svm_model = LinearSVM()\n",
    "# 定义batch_size\n",
    "batch_size = 200\n",
    "\n",
    "# 使用更多的超参数进行训练\n",
    "learning_rates = [1e-3, 1e-4, 1e-5, 1e-6, 1e-7]\n",
    "regularizations = [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4]\n",
    "num_iters = [500, 1000, 1500, 2000]\n",
    "\n",
    "results = {}\n",
    "total_configurations = len(learning_rates) * len(regularizations) * len(num_iters)\n",
    "progress_bar = tqdm(total=total_configurations, desc=\"Training progress\")\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for reg in regularizations:\n",
    "        svm = LinearSVM()\n",
    "        loss_history = []\n",
    "        train_accuracies = []\n",
    "        val_accuracies = []\n",
    "        for iters in num_iters:\n",
    "            loss_hist = svm.train(X_train_combined, y_train, learning_rate=lr, reg=reg, num_iters=iters, batch_size=batch_size, verbose=False)\n",
    "            y_train_pred = svm.predict(X_train_combined)\n",
    "            y_val_pred = svm.predict(X_val_combined)\n",
    "            train_accuracy = np.mean(y_train_pred == y_train)\n",
    "            val_accuracy = np.mean(y_val_pred == y_val)\n",
    "            loss_history.append(loss_hist[-1])\n",
    "            train_accuracies.append(train_accuracy)\n",
    "            val_accuracies.append(val_accuracy)\n",
    "            progress_bar.update(1)\n",
    "        results[(lr, reg)] = (num_iters, loss_history, train_accuracies, val_accuracies)\n",
    "\n",
    "progress_bar.close()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "948c0fb1",
   "metadata": {},
   "source": [
    "# 创建三组子图，每组对应一个指标：损失、训练准确率、验证准确率\n",
    "fig, axes = plt.subplots(3, len(learning_rates), figsize=(10 * len(learning_rates), 10 * 3))  # 使每个子图都是方形\n",
    "\n",
    "for idx_lr, lr in enumerate(learning_rates):\n",
    "    for idx_metric, metric in enumerate(['Loss', 'Train Acc', 'Val Acc']):\n",
    "        ax = axes[idx_metric, idx_lr]\n",
    "        for reg in regularizations:\n",
    "            iters, loss_history, train_accuracies, val_accuracies = results[(lr, reg)]\n",
    "            if metric == 'Loss':\n",
    "                values = loss_history\n",
    "                marker = 'o'\n",
    "                linestyle = '-'\n",
    "            elif metric == 'Train Acc':\n",
    "                values = train_accuracies\n",
    "                marker = '^'\n",
    "                linestyle = '--'\n",
    "            else:\n",
    "                values = val_accuracies\n",
    "                marker = 's'\n",
    "                linestyle = '-.'\n",
    "\n",
    "            label = f'Reg={reg}'\n",
    "            ax.plot(num_iters, values, marker=marker, linestyle=linestyle, label=label)\n",
    "        \n",
    "        ax.set_title(f'{metric} for LR={lr}')\n",
    "        ax.set_xlabel('Number of Iterations')\n",
    "        ax.set_ylabel(metric)\n",
    "\n",
    "        # 将图例放在图的外部右侧\n",
    "        ax.legend(loc='upper left', bbox_to_anchor=(1.05, 1), title='Regularization')\n",
    "\n",
    "# 调整布局以容纳图例\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(right=0.85)  # 留出更多空间在右侧给图例\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "353874e7",
   "metadata": {},
   "source": [
    "# 寻找最佳模型\n",
    "best_val_accuracy = 0\n",
    "best_svm_model = None\n",
    "best_lr = None\n",
    "best_reg = None\n",
    "best_iters = None\n",
    "\n",
    "for (lr, reg), (iters, loss_history, train_accuracies, val_accuracies) in results.items():\n",
    "    max_idx = np.argmax(val_accuracies)\n",
    "    if val_accuracies[max_idx] > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracies[max_idx]\n",
    "        best_lr = lr\n",
    "        best_reg = reg\n",
    "        best_iters = iters[max_idx]\n",
    "        best_svm_model = LinearSVM()\n",
    "        best_svm_model.train(X_train_combined, y_train, learning_rate=lr, reg=reg, num_iters=best_iters, batch_size=batch_size, verbose=False)\n",
    "\n",
    "print(f\"Best model found with LR={best_lr}, Reg={best_reg}, Iters={best_iters}, Validation Accuracy={best_val_accuracy}\")\n",
    "\n",
    "# 在测试集上评估最佳模型\n",
    "y_test_pred_hog = best_svm_model.predict(X_test_combined)\n",
    "test_accuracy_hog = np.mean(y_test_pred_hog == y_test)\n",
    "print('Test Accuracy with HOG features:', test_accuracy_hog)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b9f2abd",
   "metadata": {},
   "source": [
    "# 使用已知最佳参数训练SVM模型\n",
    "learning_rate = 1e-3\n",
    "reg = 1e-2\n",
    "num_iters = 2000\n",
    "batch_size = 200\n",
    "\n",
    "svm_model = LinearSVM()\n",
    "svm_model.train(X_train_combined, y_train, learning_rate, reg, num_iters, batch_size, verbose=True)\n",
    "\n",
    "# 保存模型到文件\n",
    "with open('svm_model_hog.pkl', 'wb') as f:\n",
    "    pickle.dump(svm_model, f)\n",
    "\n",
    "print(\"Model training complete and saved to 'svm_model_hog.pkl'.\")\n",
    "\n",
    "# 在测试集上评估模型\n",
    "y_test_pred_hog = svm_model.predict(X_test_combined)\n",
    "test_accuracy_hog = np.mean(y_test_pred_hog == y_test)\n",
    "print('Test Accuracy with HOG features:', test_accuracy_hog)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a45f8f",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
